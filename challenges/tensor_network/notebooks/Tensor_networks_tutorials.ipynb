{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color: #7f1cdb;\"><b>Tensor Networks in Quantum Computing</b></span>\n",
    "\n",
    "In the context of **quantum computing**, tensor networks have become an essential tool for simulating and analyzing large-scale quantum systems. Due to the exponential nature of the state space in many-qubit systems, directly handling such systems is computationally intractable. This is where tensor networks offer an efficient solution.\n",
    "\n",
    "**Tensor networks** allow for compressed representations of quantum states, capturing the entanglement structure of qubits and enabling simulations and calculations with reduced computational costs. Key applications of tensor networks in quantum computing include:\n",
    "\n",
    "- **Quantum circuit simulation**: Tensor networks are used to efficiently simulate quantum circuits with thousands of qubits.\n",
    "- **Entanglement analysis**: They help study and understand quantum entanglement in many-body systems.\n",
    "- **Quantum algorithms**: Tensor networks are useful for the design and optimization of quantum algorithms, such as variational approximation algorithms.\n",
    "\n",
    "In particular, tensor architectures like **MPS (Matrix Product States)** and **PEPS (Projected Entangled Pair States)** are widely used to represent many-body quantum states. These networks enable the simulation of quantum systems with large numbers of qubits by reducing the dimensionality of the problem and efficiently contracting the tensors that represent quantum operations.\n",
    "\n",
    "The use of tensor networks has proven to be an effective strategy for addressing the computational challenges posed by quantum simulation, and it remains an active area of research in quantum computing.\n",
    "\n",
    "\n",
    "## <span style=\"color: #e6023e;\"><b>Matrix Product States (MPS)</b></span>\n",
    "\n",
    "**Matrix Product States (MPS)** are one of the most fundamental structures in tensor networks. They provide a compact and efficient representation of quantum states, especially for one-dimensional systems with limited quantum entanglement. MPS are widely used in quantum physics and quantum computing to describe many-body systems.\n",
    "\n",
    "In an MPS, the full quantum state of a system is expressed as a product of matrices (tensors) along each site or qubit. Instead of explicitly storing an exponentially large quantum state, MPS decompose the state into a series of tensors connected by shared \"bond dimensions,\" significantly reducing the storage and computational complexity.\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://imgur.com/hJkQlWZ.png\" width=400 /></p>\n",
    "\n",
    "### <span style=\"color: #3b23ff;\"><b>Internal Dimension (Bond Dimension)</b></span>\n",
    "\n",
    "The **bond dimension** (denoted by $\\chi$) is a key parameter in MPS. It refers to the size of the matrices connecting neighboring tensors and determines how much entanglement can be captured between different parts of the quantum system. The larger the bond dimension, the more entanglement the MPS can represent.\n",
    "\n",
    "- **Low bond dimension**: When the bond dimension $\\chi$ is small, the MPS can efficiently represent quantum states with little or no entanglement between different parts of the system. These states are often simple and can be represented with minimal computational resources.\n",
    "  \n",
    "- **High bond dimension**: As the bond dimension $\\chi$ increases, the MPS can represent more complex quantum states with higher levels of entanglement. However, increasing the bond dimension also increases the computational cost and memory required to store and manipulate the MPS.\n",
    "\n",
    "The behavior of how MPSs replicate the total Hilbert space of a quantum system as a function of its internal link dimension is shown in the following figure\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://imgur.com/qHiBTxG.png\" width=400 /></p>\n",
    "\n",
    "\n",
    "In practice, MPS are extremely useful for simulating one-dimensional quantum systems, such as spin chains, where the entanglement between different parts of the system is typically low and can be efficiently captured by an MPS with a small bond dimension. MPS are also at the heart of the **Density Matrix Renormalization Group (DMRG)** algorithm, one of the most powerful numerical techniques for studying low-dimensional quantum systems. MPS are a foundational concept in tensor networks and are an essential tool for the study and simulation of quantum systems, especially in one dimension.\n",
    "\n",
    "\n",
    "### <span style=\"color: #3b23ff;\"><b>Supplementary material</b></span>\n",
    "\n",
    "\n",
    "- <span style=\"color: #FFA500;\">A Practical Introduction to Tensor Networks</span> [click here](https://arxiv.org/abs/1306.2164)\n",
    "\n",
    "- <span style=\"color: #FFA500;\">A Practical Guide to the Numerical Implementation of Tensor Networks I</span> [click here](https://arxiv.org/abs/2202.02138)\n",
    "\n",
    "- <span style=\"color: #FFA500;\">Tensor Network web MPS</span> [click here](https://tensornetwork.org/mps/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color: #7f1cdb;\"><b>Tensor Networks Tutorials</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #e6023e;\"><b>Library</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from ncon import ncon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #e6023e;\"><b>Tensor contractions</b></span>\n",
    "\n",
    "### <span style=\"color: #3b23ff;\"><b>Different ways to initialize a tensor</b></span>\n",
    "\n",
    "We explore the different ways in which a tensor can be defined in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random integer tensor of rank 3 and dimensions: (2, 3, 4)\n",
    "\n",
    "A = np.random.rand(2,3,4)\n",
    "\n",
    "# Rank 2 and 5x5 identity matrix\n",
    "\n",
    "B = np.eye(5,5)\n",
    "\n",
    "# Tensor of 1 of rank 4 of dimensions: (2, 4, 2, 4)\n",
    "\n",
    "C = np.ones((2,4,2,4))\n",
    "\n",
    "# Matrix of zeros of rank 2 and dimensions: (3, 5)\n",
    "\n",
    "D = np.zeros((3,5))\n",
    "\n",
    "# Complex tensor of rank 3 and dimensions: (2, 3, 4)\n",
    "\n",
    "E = np.random.rand(2,3,4) + 1j*np.random.rand(2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #3b23ff;\"><b>Permutation and reshaping operations</b></span>\n",
    "\n",
    "We explore the different operations that can be applied to a generic tensor to change its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor A of rank 4 of dimensions: (4, 4, 4, 4, 4)\n",
    "\n",
    "A = np.random.rand(4,4,4,4)\n",
    "\n",
    "# Tensor A with permuted indices (0, 1, 2, 3) --> (3, 0, 1, 2)\n",
    "\n",
    "Atilda = A.transpose(3,0,1,2)\n",
    "\n",
    "# Tensor reordered to matrix\n",
    "\n",
    "B = np.random.rand(4,4,4)\n",
    "\n",
    "# Matrix obtained from the tensor after grouping indices\n",
    "\n",
    "Btilda = B.reshape(4,4**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #3b23ff;\"><b>Binary tensor contractions</b></span>\n",
    "\n",
    "A fundamental operation within tensor networks consists of contracting tensors with each other to generate tensors of new ranges. An example of the contraction of two tensors to generate a new tensor is shown below.\n",
    "\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://imgur.com/e5ADY3m.png\" width=900 /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10\n",
    "A = np.random.rand(d,d,d,d)  \n",
    "B = np.random.rand(d,d,d,d)\n",
    "\n",
    "# Reorder indexes\n",
    "\n",
    "Ap = A.transpose(0,2,1,3)\n",
    "Bp = B.transpose(0,3,1,2)\n",
    "\n",
    "# We group indexes\n",
    "\n",
    "App = Ap.reshape(d**2,d**2)\n",
    "Bpp = Bp.reshape(d**2,d**2)\n",
    "\n",
    "# We contract tensor\n",
    "\n",
    "Cpp = App @ Bpp\n",
    "\n",
    "# Ungroup and recover the desired rank tensor\n",
    "            \n",
    "C = Cpp.reshape(d,d,d,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #3b23ff;\"><b>Contraction of tensor networks</b></span>\n",
    "\n",
    "A generalization of the previous example consists of the contraction of a large tensor network to give rise to a single equivalent tensor.\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://imgur.com/1ZK7GVm.png\" width=400 /></p>\n",
    "\n",
    "To perform the contraction operations, the network links are usually labeled in order to sort and clarify the contraction operations.\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://imgur.com/GA2S7vA.png\" width=300 /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the internal dimensions\n",
    "\n",
    "d = 10\n",
    "\n",
    "# we define the random tensors\n",
    "\n",
    "A = np.random.rand(d,d,d)\n",
    "B = np.random.rand(d,d,d,d)\n",
    "C = np.random.rand(d,d,d)\n",
    "D = np.random.rand(d,d)\n",
    "\n",
    "# We implement the shrinkage operation by using the ncon function\n",
    "\n",
    "TensorArray = [A,B,C,D]\n",
    "\n",
    "IndexArray = [[1,-2,2],[-1,1,3,4],[5,3,2],[4,5]]\n",
    "\n",
    "E = ncon(TensorArray,IndexArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #e6023e;\"><b>Tensor Decompositions</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #3b23ff;\"><b>Tensor decomposition with SVD</b></span>\n",
    "\n",
    "Application of the SVD method to a generic tensor and obtaining the tensor after the application of the SVD method without truncation and with truncation.\n",
    "\n",
    "<p style=\"text-align: center\"><img src=\"https://imgur.com/2mRpwUa.png\" width=400 /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between Af and A: 1.6799247033380653e-13\n"
     ]
    }
   ],
   "source": [
    "# We define the dimension\n",
    "\n",
    "d = 10\n",
    "\n",
    "# We generate a rank 3 tensor\n",
    "\n",
    "A = np.random.rand(d,d,d)\n",
    "\n",
    "# We regroup the indices of the tensor to transform it into a matrix for the SVD.\n",
    "\n",
    "Am = A.reshape(d**2,d)\n",
    "\n",
    "# We apply the SVD method\n",
    "\n",
    "Um, Sm, Vh = LA.svd(Am,full_matrices=False)\n",
    "\n",
    "# We convert U back to tensor\n",
    "\n",
    "U = Um.reshape(d,d,d) \n",
    "\n",
    "# We create the diagonal matrix of singular values\n",
    "\n",
    "S = np.diag(Sm)\n",
    "\n",
    "# We contract tensor\n",
    "\n",
    "Af = ncon([U,S,Vh],[[-1,-2,1],[1,2],[2,-3]])\n",
    "dA = LA.norm(Af-A)\n",
    "\n",
    "print('Overlap between Af and A:', dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between Af and A: 18.2374910567433\n",
      "Overlap between Af and A: 8.84444852511017\n",
      "Overlap between Af and A: 8.051872282692267\n",
      "Overlap between Af and A: 7.253070240576398\n",
      "Overlap between Af and A: 6.583090583064308\n",
      "Overlap between Af and A: 5.874558690817121\n",
      "Overlap between Af and A: 5.130084694750547\n",
      "Overlap between Af and A: 4.28718390251161\n",
      "Overlap between Af and A: 3.3846043133142483\n",
      "Overlap between Af and A: 2.3576023894617384\n",
      "Overlap between Af and A: 5.018703747881979e-14\n"
     ]
    }
   ],
   "source": [
    "# We perform the same procedure but generate a truncation by applying the SVD method to reduce the dimensionality of the system.\n",
    "# We define the dimension\n",
    "\n",
    "d = 10\n",
    "\n",
    "# We generate a rank 3 tensor\n",
    "\n",
    "A = np.random.rand(d,d,d)\n",
    "\n",
    "# We regroup the indices of the tensor to transform it into a matrix for the SVD.\n",
    "\n",
    "Am = A.reshape(d**2,d)\n",
    "\n",
    "# We truncate the matrix S to reduce the matrix dimension\n",
    "\n",
    "for j in range(d + 1):\n",
    "    \n",
    "    Um, Sm, Vh = LA.svd(Am,full_matrices=False)\n",
    "\n",
    "    for i in range(j, len(Sm)):\n",
    "        Sm[i] = 0\n",
    "\n",
    "    U = Um.reshape(d,d,d) \n",
    "    S = np.diag(Sm)\n",
    "\n",
    "    # We contract tensor\n",
    "\n",
    "    Af = ncon([U,S,Vh],[[-1,-2,1],[1,2],[2,-3]])\n",
    "    dA = LA.norm(Af-A)\n",
    "\n",
    "    print('Overlap between Af and A:', dA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
